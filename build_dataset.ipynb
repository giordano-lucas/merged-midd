{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from os import walk\n",
    "import pandas as pd\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read all MIDD data files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def midd_files(DIRECTORY = 'midd', n_layouts=4):\n",
    "   \"\"\"\n",
    "   Generator on all files in the MIDD datasets (all layouts)\n",
    "   \n",
    "   return a generator of tuples [(str,str,int)] : (\n",
    "      name of the file,\n",
    "      relative path to the file,\n",
    "      id of the layout of the file\n",
    "      )\n",
    "   \"\"\"\n",
    "   def all_files_in_dir(directory):\n",
    "      \"\"\"return all file names stored in the specified directory\"\"\"\n",
    "      filenames = next(walk(directory), (None, None, []))[2]  # [] if no file\n",
    "      return filenames\n",
    "   # iterate over layouts\n",
    "   for layout_id in range(n_layouts):\n",
    "      layout_dir = f'{DIRECTORY}/layout{layout_id}'\n",
    "      for file_name in all_files_in_dir(layout_dir):\n",
    "         file_path = f\"{layout_dir}/{file_name}\"\n",
    "         yield file_name.split(\".\")[0], file_path, layout_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conversion of string fields to interger"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class IdMapper():\n",
    "   def __init__(self):\n",
    "      def gen_id(): # simple integer generator\n",
    "         id = -1\n",
    "         while True:\n",
    "            id = id + 1\n",
    "            yield id\n",
    "      self.gen_id = gen_id()\n",
    "      self.map = {}\n",
    "      self.is_built = False\n",
    "\n",
    "   def add_item(self,item:str):\n",
    "      if self.is_built:\n",
    "         raise ValueError('Cannot call add_item on a built IdMapper')\n",
    "      if item not in self.map.keys():\n",
    "         id = next(self.gen_id)\n",
    "         self.map[item]=id\n",
    "      return self.map[item]\n",
    "\n",
    "   def build(self):\n",
    "      if self.is_built:\n",
    "         raise ValueError('IdMapper is already built')\n",
    "      self.is_built = True\n",
    "      return self.map\n",
    "\n",
    "class Metadata():\n",
    "   def __init__(self,convert=False):\n",
    "      self.ner_tags = IdMapper()\n",
    "      self.convert  = convert\n",
    "\n",
    "   def add(self, record):\n",
    "      if self.convert:\n",
    "         converted_ner_tag = []\n",
    "      for tag in record['ner_tag']:\n",
    "         tag_id = self.ner_tags.add_item(tag)\n",
    "         if self.convert:\n",
    "            converted_ner_tag.append(tag_id)\n",
    "\n",
    "      if self.convert:\n",
    "         record['ner_tag'] = converted_ner_tag\n",
    "      return record\n",
    "\n",
    "   def build(self):\n",
    "      return {\n",
    "         'ner_tag' : self.ner_tags.build()\n",
    "      }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def build_dataset(convert = False):\n",
    "    \n",
    "    dataset = []\n",
    "    metadata = Metadata(convert=convert)\n",
    "    for file_name, file_path, layout_id in midd_files():\n",
    "        df = pd.read_csv(file_path)\n",
    "        record = {   \n",
    "            \"name\": file_name,\n",
    "            \"layout\":layout_id,\n",
    "            'ner_tag':df.Tag.tolist(),\n",
    "            'token':df.Text.tolist()\n",
    "        }\n",
    "        record = metadata.add(record)\n",
    "        dataset.append(record)\n",
    "\n",
    "    return metadata.build(),dataset\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def write_json(data, name, DIRECTORY='data'):\n",
    "    \"\"\"Write the files to disk in JSON format\"\"\"\n",
    "    with open(f'{DIRECTORY}/{name}.json', 'w') as file:  #open the file in write mode\n",
    "        file.write(json.dumps(data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# write both converted and non-converted version\n",
    "for convert in [True,False]:\n",
    "    meta,dataset = build_dataset(convert=convert)\n",
    "    DIRECTORY=f\"data/{'converted' if convert else 'simple'}\"\n",
    "    write_json(meta,'meta',DIRECTORY=DIRECTORY)\n",
    "    write_json(dataset,'midd',DIRECTORY=DIRECTORY)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "meta,dataset = build_dataset(convert=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "list(meta['ner_tag'].keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-SUPP_N',\n",
       " 'I-SUPP_N',\n",
       " 'O',\n",
       " 'B-INV_L',\n",
       " 'I-INV_L',\n",
       " 'B-INV_NO',\n",
       " 'I-INV_NO',\n",
       " 'B-INV_DL',\n",
       " 'B-INV_DT',\n",
       " 'B-BUY_N',\n",
       " 'I-BUY_N',\n",
       " 'B-GSTL',\n",
       " 'I-GSTL',\n",
       " 'B-BUY_G',\n",
       " 'B-SUPP_G',\n",
       " 'B-GT_AMTL',\n",
       " 'I-GT_AMTL',\n",
       " 'B-GT_AMT',\n",
       " 'I-SUPP_G',\n",
       " '18',\n",
       " 'E',\n",
       " '29',\n",
       " '0',\n",
       " '56',\n",
       " 'I-INV_DL',\n",
       " '2nd',\n",
       " '300.77',\n",
       " '385',\n",
       " 'I-GT_AMT']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train / Test split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from  sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "for convert in [True,False]:\n",
    "    _,dataset = build_dataset(convert=convert)\n",
    "    DIRECTORY=f\"data/{'converted' if convert else 'simple'}\"\n",
    "    train, test = train_test_split(\n",
    "        dataset, \n",
    "        shuffle=True, \n",
    "        test_size=0.25\n",
    "    )\n",
    "    write_json(train,'train',DIRECTORY=DIRECTORY)\n",
    "    write_json(test,'test',DIRECTORY=DIRECTORY)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "31fe91183f8a5c74c797c6b4c276a92094f2c478f4dde77e20611e1e67ba7fd3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}